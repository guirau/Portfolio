{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split, Subset\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from oma_recipeclassifier.src.model_dev.few_shot.balanced_batch_sampler import BalancedBatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    SAVE_INTERVAL = 9\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 10\n",
    "\n",
    "    # GPT - Inconsistencies in handling model precision\n",
    "    def convert_models_to_fp32(model):\n",
    "        for p in model.parameters():\n",
    "            p.data = p.data.float()\n",
    "            if p.requires_grad():\n",
    "                p.grad.data = p.grad.data.float()\n",
    "\n",
    "    # ----------\n",
    "    # Load model \n",
    "    # ----------\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False) # jit=False to disable TorchScript for fine-tuning\n",
    "    # GPT - These lines are unnecessary?\n",
    "    if device == \"cpu\":\n",
    "        model.float() # Converts model to fp32\n",
    "    else:\n",
    "        clip.model.convert_weights(model) # Converts model to fp16 (unnecessary since CLIP already uses fp16 by default)\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "    weights_path = Path(\"model_checkpoints\") # Dir to save model checkpoints\n",
    "    weights_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # ----------\n",
    "    # Load dataset \n",
    "    # ----------\n",
    "    dataset = torchvision.datasets.ImageFolder(\"dataset\", transform=preprocess)\n",
    "    \n",
    "    # Split dataset into train/test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_indices, test_indices = random_split(range(len(dataset)), [train_size, test_size])\n",
    "    train_dataset, test_dataset = Subset(dataset, train_indices), Subset(dataset, test_indices)\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Extract train/test labels\n",
    "    train_labels = torch.tensor([dataset.targets[i] for i in train_indices])\n",
    "    test_labels = torch.tensor([dataset.targets[i] for i in test_indices])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "    test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_sampler)\n",
    "    \n",
    "    # Display train/test dataset distribution\n",
    "    \n",
    "    # ----------\n",
    "    # Training \n",
    "    # ----------\n",
    "    # Loss functions\n",
    "    loss_img = torch.nn.CrossEntropyLoss()\n",
    "    loss_txt = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=1e-7, weight_decay=1e-4)\n",
    "    \n",
    "    num_batches_train = len(train_dataloader.dataset) / BATCH_SIZE\n",
    "    num_batches_test = len(test_dataloader.dataset) / BATCH_SIZE\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        epoch_train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, total=num_batches_train):\n",
    "            optimizer.zero_grad() # Clear gradients from previous iteration\n",
    "            \n",
    "            images, label_ids = batch\n",
    "            \n",
    "            images = torch.stack([img for img in images], dim=0).to(device) # Stack images into a single tensor (adds an extra dim representing the batch)\n",
    "            # Generate text prompts: the number of text prompts will be equal to the number of images in the batch (label_ids)\n",
    "            # Scenario: matching each image with its corresponding text prompt, doesn't allow for comparison against other text prompts\n",
    "            # texts = [f\"A photo of a {train_dataset.dataset.classes[label_id]}\" for label_id in label_ids]\n",
    "            texts = [f\"A photo of a {dataset.classes[label_id]}\" for label_id in label_ids]\n",
    "            text = clip.tokenize(texts).to(device) # Tokenize text prompts\n",
    "            \n",
    "            print(f\"images shape: {images.shape}\")\n",
    "            print(f\"label_ids shape: {label_ids.shape}\")\n",
    "            print(f\"label_ids: {label_ids}\")\n",
    "            print(f\"text shape: {text.shape}\")\n",
    "            \n",
    "            logits_per_image, logits_per_text = model(images, text) # Forward pass\n",
    "    \n",
    "            # Ground truth labels: For each batch, the i-th image corresponds to the i-th text\n",
    "            # Therefore, the i-th image should have the same label as the i-th text, i.e. [0, 1, 2, ..., BATCH_SIZE - 1]\n",
    "            # The same happens for text, so we use the same ground truth for both image and text\n",
    "            ground_truth = torch.arange(logits_per_image.shape[0], dtype=torch.long, device=device)\n",
    "            \n",
    "            print(f\"logits_per_image shape: {logits_per_image.shape}\")\n",
    "            print(f\"logits_per_text shape: {logits_per_text.shape}\")\n",
    "            print(f\"ground_truth shape: {ground_truth.shape}\")\n",
    "            print(f\"ground_truth: {ground_truth}\")\n",
    "    \n",
    "            # Compute loss\n",
    "            total_train_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "            total_train_loss.backward() # Backward pass\n",
    "            epoch_train_loss += total_train_loss\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0) # Clip gradients to prevent exploding gradients\n",
    "            \n",
    "            if device == \"cpu\":\n",
    "                optimizer.step() # Update weights\n",
    "            else:\n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step() # Update weights\n",
    "                clip.model.convert_weights(model)\n",
    "            \n",
    "        epoch_train_loss /= num_batches_train # Average loss per epoch\n",
    "        writer.add_scalar(\"Loss/train\", epoch_train_loss, epoch) # Log loss to TensorBoard\n",
    "        \n",
    "        # Save model weights\n",
    "        if epoch % SAVE_INTERVAL == 0:\n",
    "            torch.save({\"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, weights_path / f\"epoch_{epoch}.pt\")\n",
    "            print(f\"Saved weights under {weights_path}/epoch_{epoch}.pt\")\n",
    "           \n",
    "        # ----------\n",
    "        # Testing\n",
    "        # ----------\n",
    "        num_batches_test = len(test_dataloader.dataset) / BATCH_SIZE \n",
    "        epoch_test_loss = 0\n",
    "        model.eval()\n",
    "        \n",
    "        acc_top3_list = []\n",
    "        acc_top1_list = []\n",
    "        \n",
    "        print(\"------ Testing ------\")\n",
    "        num_classes = len(dataset.classes)\n",
    "        classes = torch.arange(num_classes, device=device)\n",
    "        \n",
    "        print(f\"num_classes: {num_classes}\")\n",
    "        print(f\"dataset.classes: {dataset.classes}\")\n",
    "        print(f\"classes: {classes}\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(test_dataloader, total=num_batches_test)):\n",
    "            images, label_ids = batch\n",
    "            images = images.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            \n",
    "            # Generate text prompts: the number of text prompts will be equal to the number of classes in the dataset (classes)\n",
    "            # Scenario: classifying each image against all possible classes, allows for comparison against all classes\n",
    "            texts = torch.cat([clip.tokenize(f\"A photo of a {c}\") for c in dataset.classes]).to(device) # Concatenate text prompts\n",
    "            \n",
    "            print(f\"images shape: {images.shape}\")\n",
    "            print(f\"label_ids shape: {label_ids.shape}\")\n",
    "            print(f\"label_ids: {label_ids}\")\n",
    "            print(f\"texts shape: {texts.shape}\")\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(texts)\n",
    "                \n",
    "                logits_per_image, logits_per_text = model(images, texts) # Forward pass\n",
    "                \n",
    "                ground_truth_img = torch.arange(logits_per_image.shape[0], dtype=torch.long, device=device)\n",
    "                # Ground truth labels for text\n",
    "                ground_truth_txt = -1 * torch.ones(len(classes), dtype=torch.long, device=device)\n",
    "                for idx, class_label in enumerate(classes):\n",
    "                    if class_label in label_ids:\n",
    "                        ground_truth_txt[idx] = (label_ids == class_label).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "                print(f\"logits_per_image shape: {logits_per_image.shape}\")\n",
    "                print(f\"logits_per_text shape: {logits_per_text.shape}\")\n",
    "                print(f\"ground_truth shape: {ground_truth.shape}\")\n",
    "                print(f\"ground_truth_img: {ground_truth_img}\")\n",
    "                print(f\"ground_truth_txt: {ground_truth_txt}\")\n",
    "                \n",
    "                # Compute loss\n",
    "                img_loss = loss_img(logits_per_image, ground_truth_img)\n",
    "                txt_loss = loss_txt(logits_per_text, ground_truth_txt)\n",
    "                total_loss = (img_loss + txt_loss) / 2\n",
    "                epoch_test_loss += total_loss\n",
    "            \n",
    "            # Normalize features\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            assert torch.equal(logits_per_image.T, logits_per_text), \"Logits are not equal\"\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Compute top accuracy\n",
    "            acc_top1 = torchmetrics.functional.accuracy(similarity, label_ids, task=\"multiclass\", num_classes=num_classes)\n",
    "            acc_top3 = torchmetrics.functional.accuracy(similarity, label_ids, task=\"multiclass\", num_classes=num_classes, top_k=3)\n",
    "            \n",
    "            acc_top1_list.append(acc_top1)\n",
    "            acc_top3_list.append(acc_top3)\n",
    "\n",
    "        epoch_test_loss /= num_batches_test # Average loss per epoch\n",
    "        writer.add_scalar(\"Loss/test\", epoch_test_loss, epoch) # Log loss to TensorBoard\n",
    "\n",
    "        print(f\"Epoch {epoch} train loss: {epoch_train_loss / num_batches_train}\")\n",
    "        print(f\"Epoch {epoch} test loss: {epoch_test_loss / num_batches_test}\")\n",
    "        \n",
    "        # Compute mean top3 and top1 accuracy\n",
    "        mean_top3_accuracy = torch.stack(acc_top3_list).mean().cpu().numpy()\n",
    "        print(f\"Mean Top 3 Accuracy: {mean_top3_accuracy*100:.2f}%\")\n",
    "        writer.add_scalar(\"Test Accuracy/Top3\", mean_top3_accuracy, epoch)\n",
    "        mean_top1_accuracy = torch.stack(acc_top1_list).mean().cpu().numpy()\n",
    "        print(f\"Mean Top 1 Accuracy: {mean_top1_accuracy*100:.2f}%\")\n",
    "        writer.add_scalar(\"Test Accuracy/Top1\", mean_top1_accuracy, epoch)\n",
    "        \n",
    "    writer.flush() # Make sure all pending writes are completed\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 74\n",
      "Test dataset size: 19\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 7,  6,  8,  9, 10,  1,  4,  5,  3,  0,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:05<00:06,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  1,  5,  2,  0,  6,  7,  9,  3, 10,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:11<00:01,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 2,  4,  6,  8,  3,  7,  9,  1,  0, 10,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:16,  5.30s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 5,  9,  7,  6,  3, 10,  8,  4,  2,  1,  0])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:21,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  0,  6,  4,  5,  3,  8,  1,  2,  7, 10])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:27,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8, 10,  7,  5,  0,  3,  6,  2,  4,  1,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:32,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights under model_checkpoints/epoch_0.pt\n",
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([7, 3, 6, 4, 5, 0, 8])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.guirau/GitHub/online-marketing-automation/.venv/lib/python3.12/site-packages/tqdm/std.py:636: TqdmWarning: clamping frac to range [0, 1]\n",
      "  full_bar = Bar(frac,\n",
      "168%|██████████| 1/0.59375 [00:03<-1:59:59,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 5, -1, -1,  1,  3,  4,  2,  0,  6, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([5, 7, 4, 3, 8, 6, 0])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.82s/it]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 6, -1, -1,  3,  2,  0,  5,  1,  4, -1, -1])\n",
      "Epoch 0 train loss: 1.845449686050415\n",
      "Epoch 0 test loss: 14.577085494995117\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 50.00%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  8,  7,  4,  6,  0,  5,  9, 10,  3,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:06<00:08,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 7, 10,  1,  6,  0,  5,  9,  3,  4,  8,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:12<00:01,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 2,  7, 10,  4,  0,  1,  3,  8,  5,  6,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:16,  5.32s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 2,  4,  1,  3, 10,  0,  8,  7,  5,  6,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:20,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  2,  4,  0,  8,  5,  7,  9,  3,  6, 10])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:26,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 7,  9,  1,  3,  2,  0,  4,  8,  5,  6, 10])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:30,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([7, 8, 0, 6, 4, 5, 3])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 2, -1, -1,  6,  4,  5,  3,  0,  1, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([6, 8, 4, 0, 3, 7, 5])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.46s/it]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 3, -1, -1,  4,  2,  6,  0,  5,  1, -1, -1])\n",
      "Epoch 1 train loss: 1.6538504362106323\n",
      "Epoch 1 test loss: 15.908926010131836\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 42.86%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 3,  1, 10,  8,  9,  7,  6,  5,  2,  0,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:04<00:05,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  2,  5,  4,  6,  7,  8,  3,  0, 10,  1])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:09<00:01,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 0,  9,  4,  8, 10,  1,  5,  6,  2,  3,  7])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:13,  4.48s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 0,  1,  4,  8,  6,  9,  3,  7,  2, 10,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:17,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([10,  3,  7,  5,  4,  2,  8,  6,  0,  1,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:22,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  3,  8,  7,  5,  9,  6,  2, 10,  4,  0])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:26,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([8, 4, 3, 6, 7, 0, 5])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 5, -1, -1,  2,  1,  6,  3,  4,  0, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([3, 5, 0, 7, 4, 8, 6])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  3.00s/it]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 2, -1, -1,  0,  4,  1,  6,  3,  5, -1, -1])\n",
      "Epoch 2 train loss: 1.428858995437622\n",
      "Epoch 2 test loss: 17.54129981994629\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 57.14%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  4,  0,  7,  6, 10,  8,  5,  2,  9,  3])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:06<00:08,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  3,  4,  9,  0,  5, 10,  6,  1,  7,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:10<00:01,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  0,  2,  4, 10,  1,  9,  6,  3,  7,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  4.84s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 7,  6,  2, 10,  0,  5,  9,  1,  4,  8,  3])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:19,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  3,  9,  6,  7,  5,  2,  8,  0, 10,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:23,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 0,  8,  5,  7,  6,  1,  2,  4, 10,  3,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:28,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([5, 4, 7, 8, 6, 0, 3])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<-1:59:59,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 5, -1, -1,  6,  1,  0,  4,  2,  3, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([5, 8, 6, 4, 0, 3, 7])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.64s/it]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 4, -1, -1,  5,  3,  0,  2,  6,  1, -1, -1])\n",
      "Epoch 3 train loss: 1.4161595106124878\n",
      "Epoch 3 test loss: 17.38113784790039\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 71.43%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 5,  1,  2,  8,  9,  0, 10,  7,  4,  6,  3])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:05<00:07,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  5,  4,  7,  2,  1,  6,  3, 10,  0,  8])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:09<00:01,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  5,  2,  7,  8,  4,  1,  6, 10,  0,  3])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  5.28s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 6,  5,  0,  7,  8, 10,  2,  3,  1,  4,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:20,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  0,  9,  5,  8,  4,  6, 10,  7,  3,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:26,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  6,  4,  7,  1,  5,  3,  2, 10,  8,  0])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:31,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([5, 8, 0, 3, 4, 7, 6])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 2, -1, -1,  3,  4,  0,  6,  5,  1, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([6, 7, 8, 3, 5, 0, 4])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.40s/it]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 5, -1, -1,  3,  6,  4,  0,  1,  2, -1, -1])\n",
      "Epoch 4 train loss: 1.2371553182601929\n",
      "Epoch 4 test loss: 11.927498817443848\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 78.57%\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  4,  6,  3,  2,  7,  5,  0,  1, 10,  8])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:04<00:06,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  0, 10,  2,  9,  7,  4,  6,  8,  3,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:08<00:01,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 3,  7,  2,  4,  8,  1,  0,  6, 10,  5,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:13,  4.47s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1,  0,  4,  2,  3,  9,  5,  7,  8,  6, 10])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:18,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 5,  4,  0, 10,  3,  6,  9,  2,  1,  8,  7])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:22,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  5,  9,  7,  1,  4,  3,  2,  0, 10,  6])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:27,  4.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([4, 7, 3, 8, 0, 5, 6])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 4, -1, -1,  2,  0,  5,  6,  1,  3, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([8, 6, 3, 5, 4, 0, 7])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.45s/it]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 5, -1, -1,  2,  4,  3,  1,  6,  0, -1, -1])\n",
      "Epoch 5 train loss: 1.2686586380004883\n",
      "Epoch 5 test loss: 13.416112899780273\n",
      "Mean Top 3 Accuracy: 92.86%\n",
      "Mean Top 1 Accuracy: 78.57%\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 3,  9,  8,  6,  5,  2,  0,  4,  7,  1, 10])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:04<00:06,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 3,  5,  7,  9, 10,  8,  1,  6,  2,  0,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:09<00:01,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 6,  3,  9,  7,  2,  0,  5,  1,  8, 10,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:13,  4.44s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 3,  5,  1,  7,  0,  6,  4, 10,  8,  2,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:19,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 6,  0,  2,  1, 10,  7,  4,  5,  8,  3,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:23,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  0,  3,  7,  6,  4, 10,  1,  2,  8,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:28,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([6, 7, 8, 3, 5, 4, 0])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 6, -1, -1,  3,  5,  4,  0,  1,  2, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([6, 5, 3, 7, 4, 8, 0])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.74s/it]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 6, -1, -1,  2,  4,  1,  0,  3,  5, -1, -1])\n",
      "Epoch 6 train loss: 1.1103264093399048\n",
      "Epoch 6 test loss: 12.412967681884766\n",
      "Mean Top 3 Accuracy: 100.00%\n",
      "Mean Top 1 Accuracy: 85.71%\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 2,  7,  9,  8,  0,  6, 10,  3,  1,  5,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:05<00:07,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  2,  5,  6,  4,  1,  7,  0,  9,  3, 10])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:10<00:01,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  7,  5, 10,  2,  3,  8,  0,  4,  1,  6])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  5.13s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  4,  9,  0,  5,  3,  1,  6,  2, 10,  7])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:19,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  2,  6,  7,  4, 10,  1,  0,  8,  3,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:23,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 1, 10,  2,  8,  4,  3,  7,  9,  6,  0,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:28,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([4, 5, 8, 6, 0, 3, 7])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<-1:59:59,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 4, -1, -1,  5,  0,  1,  3,  6,  2, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([6, 7, 5, 3, 4, 8, 0])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.45s/it]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 6, -1, -1,  3,  4,  2,  0,  1,  5, -1, -1])\n",
      "Epoch 7 train loss: 0.960955023765564\n",
      "Epoch 7 test loss: 14.409374237060547\n",
      "Mean Top 3 Accuracy: 100.00%\n",
      "Mean Top 1 Accuracy: 85.71%\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 9,  2,  1,  4,  7,  8,  3,  6,  5, 10,  0])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:05<00:07,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 3,  5,  1,  2,  4,  9,  6,  8, 10,  0,  7])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:11<00:01,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 5,  8,  6,  3,  9,  0,  1, 10,  7,  2,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:16,  5.17s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([10,  7,  6,  4,  5,  9,  8,  0,  3,  1,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:20,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 0,  7,  9,  8,  4, 10,  6,  1,  5,  2,  3])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:25,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 0,  4,  2,  9,  7,  8,  1,  5, 10,  6,  3])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:29,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([7, 5, 4, 3, 8, 0, 6])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:02<-1:59:59,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 5, -1, -1,  3,  2,  1,  6,  0,  4, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([6, 8, 0, 3, 4, 5, 7])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.44s/it]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 2, -1, -1,  3,  4,  5,  0,  6,  1, -1, -1])\n",
      "Epoch 8 train loss: 0.922077476978302\n",
      "Epoch 8 test loss: 12.589924812316895\n",
      "Mean Top 3 Accuracy: 100.00%\n",
      "Mean Top 1 Accuracy: 92.86%\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2.3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 6,  4, 10,  5,  2,  1,  3,  8,  7,  0,  9])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1/2.3125 [00:05<00:07,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 6, 10,  4,  8,  0,  2,  9,  7,  1,  3,  5])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 2/2.3125 [00:10<00:01,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 7,  3,  2,  8,  5, 10,  6,  4,  9,  1,  0])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:15,  5.05s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([10,  5,  1,  8,  3,  9,  6,  4,  7,  0,  2])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:19,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 2,  7,  1, 10,  8,  3,  5,  6,  0,  9,  4])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:23,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([11, 3, 224, 224])\n",
      "label_ids shape: torch.Size([11])\n",
      "label_ids: tensor([ 8,  5,  7, 10,  3,  4,  6,  1,  9,  2,  0])\n",
      "text shape: torch.Size([11, 77])\n",
      "logits_per_image shape: torch.Size([11, 11])\n",
      "logits_per_text shape: torch.Size([11, 11])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:28,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights under model_checkpoints/epoch_9.pt\n",
      "------ Testing ------\n",
      "num_classes: 11\n",
      "dataset.classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/0.59375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([4, 7, 0, 8, 3, 5, 6])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168%|██████████| 1/0.59375 [00:03<-1:59:59,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 2, -1, -1,  4,  0,  5,  6,  1,  3, -1, -1])\n",
      "images shape: torch.Size([7, 3, 224, 224])\n",
      "label_ids shape: torch.Size([7])\n",
      "label_ids: tensor([8, 0, 7, 5, 3, 6, 4])\n",
      "texts shape: torch.Size([11, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:06,  3.22s/it]                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_per_image shape: torch.Size([7, 11])\n",
      "logits_per_text shape: torch.Size([11, 7])\n",
      "ground_truth shape: torch.Size([11])\n",
      "ground_truth_img: tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "ground_truth_txt: tensor([ 1, -1, -1,  4,  6,  3,  5,  2,  0, -1, -1])\n",
      "Epoch 9 train loss: 0.8230394124984741\n",
      "Epoch 9 test loss: 15.124711036682129\n",
      "Mean Top 3 Accuracy: 100.00%\n",
      "Mean Top 1 Accuracy: 85.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted keyword: chopping-board with probability 44.89%\n"
     ]
    }
   ],
   "source": [
    "# Model inference\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Load the fine-tuned weights\n",
    "checkpoint_path = Path(\"model_checkpoints/epoch_9.pt\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Load image\n",
    "image_path = os.path.join(\"test_imgs/test_group.jpg\")\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess image\n",
    "image = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate text prompt\n",
    "keywords = [\"chopping-board\", \"glass-bowl-large\", \"glass-bowl-medium\", \"glass-bowl-small\", \"group_step\", \"oven-dish\", \"oven-tray\", \"pan\", \"pot-one-handle\", \"pot-two-handles-medium\", \"pot-two-handles-small\"]\n",
    "text_prompts = [f\"A photo of a {keyword}\" for keyword in keywords]\n",
    "tokenized_text = clip.tokenize(text_prompts).to(device)\n",
    "\n",
    "# Generate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True) # Normalize features\n",
    "    \n",
    "    text_features = model.encode_text(tokenized_text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # Normalize features\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Get predicted keyword\n",
    "predicted_prob, predicted_keyword_idx = similarity.topk(1, dim=-1)\n",
    "\n",
    "# Print prediction\n",
    "predicted_keyword = keywords[predicted_keyword_idx.item()]\n",
    "print(f\"Predicted keyword: {predicted_keyword} with probability {predicted_prob.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
