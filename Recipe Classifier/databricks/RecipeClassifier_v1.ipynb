{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ee88b9-8dd1-445a-b797-82d85b15f452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step Recipe Classifier\n",
    "\n",
    "#### Fine-Tuning CLIP for Image Classification\n",
    "\n",
    "This notebook implements a complete pipeline for fine-tuning OpenAI's [CLIP](https://github.com/openai/CLIP) model on step recipe images. The pipeline includes balanced data loading, augmentation, training with validation, and inference.\n",
    "\n",
    "#### Modular Components:\n",
    "\n",
    "- **TrainingConfig**: Central configuration for hyperparameters, paths, and grid search settings\n",
    "- **DataModule**: Handles dataset loading, preprocessing, and balanced batch sampling\n",
    "- **CLIPModule**: Manages model initialization and training setup\n",
    "- **CLIPTrainer**: Orchestrates training loop, validation, and metric computation\n",
    "- **Utility Functions**: Standalone tools for monitoring, saving, and loading models\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Data Management**: Custom dataset handling with balanced sampling and augmentation\n",
    "- **Model Training**: CLIP fine-tuning with early stopping and metric tracking\n",
    "- **Experiment Tracking**: MLflow integration for logging metrics and artifacts\n",
    "- **Inference**: Production-ready inference pipeline with optimized model loading\n",
    "\n",
    "*Author: Alejandro Guirau*  \n",
    "*Last Updated: February 2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bba550c-a88a-4a2d-be39-787d0b9dd1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540f5a30-59b7-44b0-9ad1-1253439f2917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44024b0-9ccd-4274-8c09-13c2f2c0210d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q torch torchmetrics torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908e5efc-8aeb-43ae-b635-a56c222d86ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Name: clip\n",
      "Version: 1.0\n",
      "Summary: \n",
      "Home-page: \n",
      "Author: OpenAI\n",
      "Author-email: \n",
      "License: \n",
      "Location: /local_disk0/.ephemeral_nfs/envs/pythonEnv-2270c8be-a312-460a-8f71-8d9ecb5723c8/lib/python3.11/site-packages\n",
      "Requires: ftfy, packaging, regex, torch, torchvision, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Install CLIP from GitHub repo\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "!pip show clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feea06bc-fac9-4f77-bc73-f951d7f6bbe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Utility Functions\n",
    "\n",
    "#### Model Utils\n",
    "\n",
    "Utility functions for model management, accuracy computation, and dataset analysis.\n",
    "\n",
    "1. `convert_models_to_fp32`: Converts parameters to 32-bit floating point format.\n",
    "\n",
    "2. `compute_per_class_accuracy`: Calculates accuracy for each class independently. Uses a confusion matrix to handle multi-class scenarios.\n",
    "\n",
    "3. `class_distribution`: Analyzes dataset balance by calculating the percentage of samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c680fa-b459-4243-87aa-4673bfe5378b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def convert_models_to_fp32(model: torch.nn.Module):\n",
    "    \"\"\"Convert model parms and grads to fp32.\"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        if p.requires_grad:\n",
    "            p.grad.data = p.grad.data.float()\n",
    "\n",
    "def compute_per_class_accuracy(pred_labels: torch.Tensor, true_labels: torch.Tensor, num_classes: int, class_names: list[str]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate per-class accuracy using a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        pred_labels: Predicted class labels\n",
    "        true_labels: Ground truth labels\n",
    "        num_classes: Total number of classes\n",
    "        class_names: List of class names\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of per-class accuracy\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    confusion_matrix = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "    confusion_matrix = confusion_matrix(pred_labels, true_labels)\n",
    "\n",
    "    # Extract true positives (diag) and total sampes per class (row sum)\n",
    "    true_positives = confusion_matrix.diag()\n",
    "    total_samples_per_class = confusion_matrix.sum(dim=-1)\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    per_class_accuracy = {\n",
    "        class_names[i]: (true_positives[i] / total_samples_per_class[i].item() * 100) \n",
    "        if total_samples_per_class[i] > 0 else float(\"nan\") # Handle division by zero\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    return per_class_accuracy\n",
    "\n",
    "def class_distribution(dataset: Dataset = None, labels: torch.Tensor = None, class_names: list[str] = None, dataset_name: str = \"\") -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Displays and returns class distribution as percentages.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to compute the distribution for.\n",
    "        labels: Tensor of labels to compute the distribution for.\n",
    "        class_names: List of class names for display.\n",
    "        dataset_name: Name of the dataset for display.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples (class_name, percentage)\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        raise ValueError(\"class_names must be provided.\")\n",
    "\n",
    "    # Count ocurrences of each class\n",
    "    if dataset is not None:\n",
    "        label_counts = Counter()\n",
    "        # Iterate through the dataset to count labels\n",
    "        for _, label in dataset:\n",
    "            label_counts[label] += 1\n",
    "        total_samples = sum(label_counts.values())\n",
    "        class_counts = [label_counts.get(i, 0) for i in range(len(class_names))]\n",
    "    elif labels is not None:\n",
    "        class_counts = torch.bincount(labels, minlength=len(class_names)).numpy()\n",
    "        total_samples = class_counts.sum()\n",
    "    else:\n",
    "        raise ValueError(\"Either dataset or labels must be provided.\")\n",
    "\n",
    "    # Create list of tuples (class_name, percentage)\n",
    "    distribution = [\n",
    "        (class_names[i], (class_counts[i] / total_samples) * 100)\n",
    "        for i in range(len(class_names))\n",
    "        if class_counts[i] > 0  # Exclude classes with zero samples\n",
    "    ]\n",
    "\n",
    "    # Sort by percentage in descending order\n",
    "    distribution.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"{dataset_name} dataset -- Class distribution:\")\n",
    "    for class_name, percentage in distribution:\n",
    "        print(f\"{class_name} -- {percentage:.2f}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5580df-390f-43e4-b5d1-f19bcf572953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Logging Utils\n",
    "\n",
    "Utility functions for visualizing model performance through confusion matrices and dataset samples. Logged as artifacts in MLflow experiments.\n",
    "\n",
    "1. `log_confusion_matrix`: Creates normalized confusion matrices to visualize model classification performance.\n",
    "\n",
    "2. `log_random_image`: Samples and visualizes random images from the datases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ca6da6-dc77-4b99-a4ab-f38304c4d811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0xfffee64b4360>\n",
      "Traceback (most recent call last):\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import mlflow\n",
    "\n",
    "def log_confusion_matrix(true_labels: torch.Tensor, pred_labels: torch.Tensor, class_names: list[str], epoch: int) -> None:\n",
    "    \"\"\"\n",
    "    Logs a normalized confusion matrix plot to MLflow.\n",
    "    \n",
    "    Args:\n",
    "        true_labels: Ground truth labels tensor\n",
    "        pred_labels: Predicted labels tensor\n",
    "        class_names: List of class names\n",
    "        epoch: Current epoch\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels.cpu().numpy(), pred_labels.cpu().numpy(), labels=range(len(class_names)))\n",
    "\n",
    "    # Normalize confusion matrix\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "        cm[np.isnan(cm)] = 0  # Replace NaN with 0\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation=90)\n",
    "\n",
    "    # Format numbers to 2 decimal places\n",
    "    for texts in ax.texts:\n",
    "        text = texts.get_text()\n",
    "        texts.set_text(f\"{float(text):.2f}\")\n",
    "\n",
    "    plt.title(f\"Confusion Matrix (Epoch {epoch})\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    \n",
    "    # Save the plot temporarily\n",
    "    temp_dir = \"/tmp/confusion_matrices\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(temp_dir, f\"{epoch:02}_confusion_matrix.png\")\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Log plot to MLflow\n",
    "    mlflow.log_artifact(plot_path, artifact_path=\"confusion_matrices\")\n",
    "\n",
    "def log_random_image(dataset: Dataset, class_names: list[str], artifact_path: str = \"augmented_images/train\", counter: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    Log a random image from the dataset with its class label.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing images and labels\n",
    "        class_names: List of class names for display\n",
    "        artifact_path: MLflow artifact diretory path\n",
    "        counter: Image counter for filename\n",
    "    \"\"\"\n",
    "    # Randomly select image\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    image, label = dataset[idx]\n",
    "\n",
    "    # Convert image to displayable format (PIL)\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = T.functional.to_pil_image(image)\n",
    "\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f\"Class: {class_names[label]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Save the plot temporarily\n",
    "    temp_dir = f\"/tmp/{artifact_path}\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(temp_dir, f\"{counter:02d}_augmented_image.png\")\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Log the image to MLflow\n",
    "    mlflow.log_artifact(plot_path, artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abfef65e-9741-43da-9f9c-9a89dd14942e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Memory Monitoring Utils\n",
    "\n",
    "Utility function for tracking GPU and CPU memory usage during model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1043fe2e-081d-4d18-a54b-867da3f1434d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def log_memory_usage(epoch, phase=\"Train\"):\n",
    "    \"\"\"Logs memory usage for debugging.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # in GB\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)  # in GB\n",
    "        print(f\"[{phase} Epoch {epoch}] GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\")\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        rss = memory_info.rss / (1024 ** 3)  # in GB\n",
    "        print(f\"[{phase} Epoch {epoch}] CPU Memory: RSS={rss:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4acd41ca-1e1b-4d75-bbe4-2c338d95d280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Early Stopping \n",
    "Implements early stopping functionality to prevent overfitting during model training.\n",
    "\n",
    "It tracks validation metrics, saves the best model checkpoint, and stops training when there hasn't been an improvement for a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53dc8d1b-5a40-4b09-8b33-eae51df8e923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to terminate training when validation metric stagnates.\n",
    "    Saves best model checkpoint and tracks improvement over epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience: int = 5, mode: str = \"min\", delta: float = 0, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize early stopping parameters.\n",
    "        \n",
    "        Args:\n",
    "            patience: How many epochs to wait before stopping.\n",
    "            mode: \"min\" for minimizing, \"max\" for maximizing.\n",
    "            delta: Minimum change to qualify as an improvement.\n",
    "            verbose: Whether to print verbose output.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_score = -float(\"inf\") if self.mode == \"max\" else float(\"inf\")\n",
    "        self.best_epoch = None\n",
    "        self.early_stop = None\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "    def __call__(self, metric: float, model: torch.nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                 path: str, epoch: int, additional_info: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Checks early stopping conditions and saves the model if metric improves.\n",
    "\n",
    "        Args:\n",
    "            metric: Current validation metric value\n",
    "            model: Model to save if metric improves\n",
    "            optimizer: Optimizer state to save\n",
    "            path: Path to save the model checkpoint\n",
    "            epoch: Current epoch number\n",
    "            additional_info (optional): Additional information to save\n",
    "        \"\"\"\n",
    "        score = metric if self.mode == \"max\" else -metric\n",
    "\n",
    "        # Check if metric improved\n",
    "        improvement = False\n",
    "        if self.mode == \"min\":\n",
    "            improvement = score < self.best_score - self.delta\n",
    "        elif self.mode == \"max\":\n",
    "            improvement = score > self.best_score + self.delta\n",
    "        else:\n",
    "            raise ValueError(f\"Mode {self.mode} is not supported. Use 'min' or 'max'.\")\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(metric, model, optimizer, path, epoch, additional_info)\n",
    "        elif improvement:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(metric, model, optimizer, path, epoch, additional_info)\n",
    "            self.epochs_no_improve = 0\n",
    "        else:\n",
    "            self.epochs_no_improve += 1\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, metric: float, model: torch.nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                        path: str, epoch: int, additional_info: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Save model when validation metric improves.\n",
    "        \n",
    "        Args:\n",
    "            metric: Current validation metric value\n",
    "            model: Model to save if metric improves\n",
    "            optimizer: Optimizer state to save\n",
    "            path: Path to save the model checkpoint\n",
    "            epoch: Current epoch number\n",
    "            additional_info (optional): Additional information to save\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"[Best model] Metric improved ({self.best_score:.4f} --> {metric:.4f}). Saving model...\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"metric\": metric,\n",
    "        }\n",
    "\n",
    "        # Add any additional info\n",
    "        if additional_info:\n",
    "            checkpoint.update(additional_info)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the early stopping variables.\"\"\"\n",
    "        self.best_score = -float(\"inf\") if self.mode == \"max\" else float(\"inf\")\n",
    "        self.best_epoch = None\n",
    "        self.epochs_no_improve = 0\n",
    "        self.early_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b3696b-3476-488b-aa4e-e073b063b67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Provides custom data augmentation and dataset oversampling functionality for CLIP fine-tuning.\n",
    "Augmentations focus on geometric transformations while preserving color information.\n",
    "\n",
    "1. `augment` function: Applies CLIP-specific data augmentation. Uses geometric transformations (crop, affine, flip, perspective), and preserves color information for CLIP compatibility\n",
    "\n",
    "2. `AugmentedOversampledDataset` class: Wraps an existing dataset to handle class imbalance, dynamically generating augmented samples for minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92ddfb3-a838-4010-9be6-17563d3e259c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def augment(image: Image.Image, seed: int = 42) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Apply data augmentation appropriate for CLIP fine-tuning.\n",
    "    \n",
    "    Focuses on geometric transformations while preserving color information\n",
    "    since CLIP's preprocessing already handles color normalization.\n",
    "    \n",
    "    Args:\n",
    "        image: Input PIL image\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Augmented PIL image\n",
    "    \"\"\"      \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Random resized crop (maintains most of original content while adding variety)\n",
    "    random_resized_crop = T.RandomResizedCrop(\n",
    "        size=(224, 224), # CLIP's input size\n",
    "        scale=(0.8, 1.0),\n",
    "        ratio=(0.9, 1.1) \n",
    "    )\n",
    "    image = random_resized_crop(image)\n",
    "\n",
    "    # RandomAffine\n",
    "    random_affine = T.RandomAffine(\n",
    "        degrees=15,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1),\n",
    "        shear=5\n",
    "    )\n",
    "    image = random_affine(image)\n",
    "\n",
    "    # Horizontal Flip (if semantically appropriate for your recipe images)\n",
    "    if random.random() > 0.5:\n",
    "        image = T.functional.hflip(image)\n",
    "\n",
    "    # Random perspective (subtle, to simulate different viewing angles)\n",
    "    perspective_transform = T.RandomPerspective(\n",
    "        distortion_scale=0.2,  # Low distortion\n",
    "        p=0.5\n",
    "    )\n",
    "    image = perspective_transform(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "class AugmentedOversampledDataset(Dataset):\n",
    "    \"\"\"Custom dataset wrapper that oversamples underrepresented classes by duplicating and augmenting their samples.\"\"\"\n",
    "    def __init__(self, subset_dataset: Dataset, original_dataset: Dataset, target_samples_per_class: int, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the augmented dataset.\n",
    "\n",
    "        Args:\n",
    "            subset_dataset: Dataset to augment.\n",
    "            original_dataset: The original ImageFolder dataset (to access class names).\n",
    "            target_samples_per_class: The target number of samples per class.\n",
    "            seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.subset_dataset = subset_dataset\n",
    "        self.original_dataset = original_dataset\n",
    "        self.target_samples_per_class = target_samples_per_class\n",
    "        self.seed = seed\n",
    "        self.augmented_samples = []\n",
    "\n",
    "        # Oversample and augment underrepresented classes\n",
    "        self._create_augmented_samples()\n",
    "\n",
    "    def _create_augmented_samples(self) -> None:\n",
    "        \"\"\"Creates augmented samples for classes with fewer than target samples.\"\"\"\n",
    "        # Group samples by class\n",
    "        samples_by_class = {class_idx: [] for class_idx in range(len(self.original_dataset.classes))}\n",
    "        for idx in self.subset_dataset.indices:\n",
    "            image, label = self.original_dataset[idx]\n",
    "            samples_by_class[label].append((image, label))\n",
    "\n",
    "        # Duplicate and augment underrepresented classes\n",
    "        for class_idx, samples in samples_by_class.items():\n",
    "            current_count = len(samples)\n",
    "            class_name = self.original_dataset.classes[class_idx]\n",
    "\n",
    "            if current_count < self.target_samples_per_class:\n",
    "                print(f\"Class {class_name}: {current_count} samples -> Target: {self.target_samples_per_class}\")\n",
    "                \n",
    "                additional_samples = []\n",
    "                for _ in range (self.target_samples_per_class - current_count):\n",
    "                    # Randomly pick a sample and apply augmentation\n",
    "                    original_image, label = random.choice(samples)\n",
    "                    augmented_image = augment(original_image, seed=self.seed)\n",
    "                    additional_samples.append((augmented_image, label))\n",
    "\n",
    "                self.augmented_samples.extend(additional_samples)\n",
    "            else:\n",
    "                print(f\"Class {class_name}: {current_count} samples (No augmentation needed)\")\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Retruns total length including original and augmented samples.\"\"\"\n",
    "        return len(self.subset_dataset) + len(self.augmented_samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[Image.Image, int]:\n",
    "        \"\"\"Returns an image/label pair from either original or augmented samples.\"\"\"\n",
    "        if idx < len(self.subset_dataset):\n",
    "            # Get original sample\n",
    "            image, label = self.original_dataset[self.subset_dataset.indices[idx]]\n",
    "        else:\n",
    "            # Get augmented sample\n",
    "            image, label = self.augmented_samples[idx - len(self.subset_dataset)]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33be80d9-2b29-44bd-ab28-fb3e290773c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training Configuration\n",
    "Module to handle configuration parameters for CLIP model training. Includes training hyperparameters, data paths, and checkpoint settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "798f099d-4f55-4e0a-965d-4c07ba7aa4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Configuration\n",
    "# -------------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for CLIP model training.\"\"\"\n",
    "    # Training parameters\n",
    "    BATCH_SIZE: int = 16\n",
    "    NUM_EPOCHS: int = 2\n",
    "    LEARNING_RATE: float = 1e-7\n",
    "    WEIGHT_DECAY: float = 1e-4\n",
    "\n",
    "    # Data parameters\n",
    "    root_dir: str = \"<PATH>\"\n",
    "    DATASET_PATH: str = f\"{root_dir}recipe_classifier/dataset/2024_11_25/\"\n",
    "    DATASET_NAME: str = \"dataset_sample\"\n",
    "\n",
    "    # Checkpoint parameters\n",
    "    SAVE_INTERVAL: int = 10\n",
    "    CHECKPOINT_SAVE_PATH: str = f\"{root_dir}recipe_classifier/checkpoints/2025_01_06/dataset_sample/\"\n",
    "    best_model_path: str = f\"{CHECKPOINT_SAVE_PATH}best_model.pth\"\n",
    "\n",
    "\n",
    "    def to_dict(self) -> dict[str, int | float]:\n",
    "        \"\"\"Convert config to dictionary for logging.\"\"\"\n",
    "        return {\n",
    "            \"BATCH_SIZE\": self.BATCH_SIZE,\n",
    "            \"NUM_EPOCHS\": self.NUM_EPOCHS,\n",
    "            \"LEARNING_RATE\": self.LEARNING_RATE,\n",
    "            \"WEIGHT_DECAY\": self.WEIGHT_DECAY,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4adda200-cc40-4bb3-a905-0d7ea03217e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Module\n",
    "Module to handle dataset operations including loading, preprocessing, augmentation, and DataLoader creation with balanced batch sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1845e72-1e07-4067-81f8-2ac2895e9b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Data Module\n",
    "# -------------\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from balanced_batch_sampler import BalancedBatchSampler\n",
    "\n",
    "class DataModule:\n",
    "    \"\"\"\n",
    "    Module for handling dataset loading, preprocessing, train/test splitting,\n",
    "    augmentation, and creation of balanced DataLoaders.\n",
    "\n",
    "    Attributes:\n",
    "        config (TrainingConfig): Configuration module containing data and training parameters.\n",
    "        dataset (torchvision.datasets.ImageFolder): The loaded dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_dataset(self, preprocess: callable) -> torchvision.datasets.ImageFolder:\n",
    "        \"\"\"\n",
    "        Load and preprocess the dataset.\n",
    "\n",
    "        Args:\n",
    "            preprocess: CLIP model preprocessing function\n",
    "\n",
    "        Returns:\n",
    "            Loaded and preprocessed ImageFolder dataset\n",
    "        \"\"\"\n",
    "        self.dataset = torchvision.datasets.ImageFolder(\n",
    "            f\"{self.config.DATASET_PATH}{self.config.DATASET_NAME}/\", transform=preprocess\n",
    "            )\n",
    "        return self.dataset\n",
    "    \n",
    "    def prepare_data(self,\n",
    "                     dataset: Dataset,\n",
    "                     train_indices: list[int],\n",
    "                     test_indices: list[int],\n",
    "                     target_samples_per_class: int = None\n",
    "                     ) -> tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        Prepare train/test datasets with optional augmentation and oversampling.\n",
    "\n",
    "        Args:\n",
    "            dataset: The complete dataset.\n",
    "            train_indices: Indices for the train dataset.\n",
    "            test_indices: Indices for the test dataset.\n",
    "            target_samples_per_class (optional): The number of samples to oversample each class. Defaults to no oversampling.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (train_dataset, test_dataset)\n",
    "        \"\"\"\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "        print(\"---- Before Augmentation ----\")\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "        # Apply augmentation and oversampling\n",
    "        if target_samples_per_class:\n",
    "            train_dataset = AugmentedOversampledDataset(train_dataset, dataset, target_samples_per_class)\n",
    "\n",
    "        print(\"\\n---- After Augmentation ----\")\n",
    "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Test dataset size: {len(test_dataset)}\\n\")\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    def get_labels(self, train_indices: list[int], test_indices: list[int]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract labels from train and test datasets.\n",
    "        \n",
    "        Args:\n",
    "            train_indices: Indices for the training dataset.\n",
    "            test_indices: Indices for the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple with labels for (train, test) datasets.\n",
    "        \"\"\"\n",
    "        train_labels = torch.tensor([self.dataset.targets[i] for i in train_indices])\n",
    "        test_labels = torch.tensor([self.dataset.targets[i] for i in test_indices])\n",
    "        return train_labels, test_labels\n",
    "    \n",
    "    def create_dataloaders(self, \n",
    "                           train_dataset: Dataset, \n",
    "                           test_dataset: Dataset, \n",
    "                           train_indices: list[int], \n",
    "                           test_indices: list[int]\n",
    "                           ) -> tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create DataLoaders with balanced batch sampling for train/test datasets.\n",
    "        \n",
    "        Args:\n",
    "            train_dataset: The train dataset.\n",
    "            test_dataset: The test dataset.\n",
    "            train_indices: Indices for the training dataset.\n",
    "            test_indices: Indices for the test dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (train_dataloader, test_dataloader)\n",
    "        \"\"\"\n",
    "        # Get labels\n",
    "        train_labels, test_labels = self.get_labels(train_indices, test_indices)\n",
    "\n",
    "        # Create samplers\n",
    "        train_sampler = BalancedBatchSampler(labels=train_labels, n_classes=self.config.BATCH_SIZE, n_samples=1)\n",
    "        test_sampler = BalancedBatchSampler(labels=test_labels, n_classes=self.config.BATCH_SIZE, n_samples=1)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)\n",
    "\n",
    "        return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66ea23b7-7751-4670-a402-d02b5f08e9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Setup Module\n",
    "Manages CLIP model initialization, configuration, and training setup including loss functions and optimizer initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0cdb25-0169-4686-9433-40afc4e4c216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Model Setup\n",
    "# -------------\n",
    "\n",
    "from pathlib import Path\n",
    "import clip\n",
    "\n",
    "class CLIPModule:\n",
    "    \"\"\"\n",
    "    Module for handling the CLIP model loading and training setup.\n",
    "\n",
    "    Attributes:\n",
    "        config (TrainingConfig): Configuration module containing data and training parameters.\n",
    "        model (torch.nn.Module): The loaded CLIP model.\n",
    "        preprocess (callable): Preprocessing function for the CLIP model.\n",
    "        device (str): The device on which the model is loaded (cuda:0 or cpu).\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.preprocess = None\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def load_model(self, model_name: str = \"ViT-B/32\") -> tuple[torch.nn.Module, callable]:\n",
    "        \"\"\"\n",
    "        Load the CLIP model and preprocessing function.\n",
    "        \n",
    "        Args:\n",
    "            model_name (optional): The name of the CLIP model to load. Defaults to \"ViT-B/32\".\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (model, preprocess_function)\n",
    "        \"\"\"\n",
    "        model, preprocess = clip.load(model_name, device=self.device, jit=False) # jit=False to disable TorchScript for fine-tuning\n",
    "\n",
    "        if self.device == \"cpu\":\n",
    "            model.float() # Converts model to fp32\n",
    "        else:\n",
    "            clip.model.convert_weights(model) # Converts model to fp16 (unnecessary since CLIP already uses fp16 by default)\n",
    "\n",
    "        # Model checkpoints\n",
    "        weights_path = Path(self.config.CHECKPOINT_SAVE_PATH)\n",
    "        weights_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        return model, preprocess\n",
    "    \n",
    "    def setup_training(self) -> tuple[torch.nn.Module, torch.nn.Module, torch.optim.AdamW]:\n",
    "        \"\"\"\n",
    "        Setup the training components: loss functions and optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (image_loss, text_loss, optimizer)\n",
    "        \"\"\"\n",
    "        # Loss functions\n",
    "        loss_img = torch.nn.CrossEntropyLoss()\n",
    "        loss_txt = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        \n",
    "        # Optimizer\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.AdamW(params, lr=self.config.LEARNING_RATE, weight_decay=self.config.WEIGHT_DECAY)\n",
    "\n",
    "        return loss_img, loss_txt, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bd0d8e-8161-4f1c-804c-ba67adce4e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Training Module\n",
    "Trainer module that handles CLIP model training, validation, metrics tracking, and MLflow logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d5dd0d-f8d6-403b-af1a-4c2a3d4b1382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Model Trainer\n",
    "# -------------\n",
    "\n",
    "import mlflow\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CLIPTrainer:\n",
    "    \"\"\"\n",
    "    Module with methods for training the CLIP model.\n",
    "\n",
    "    Manages the training loop, validation, metric computation, checkpointing,\n",
    "    and experiment tracking via MLflow.\n",
    "\n",
    "    Attributes:\n",
    "        config: Training configuration module\n",
    "        model_module: CLIP model and preprocessing module\n",
    "        data_module: Data loading module\n",
    "        device: Device for training (GPU/CPU)\n",
    "        best_accuracy: Best accuracy achieved during training\n",
    "        early_stopping: Early stopping handler\n",
    "    \"\"\"\n",
    "    def __init__(self, config: TrainingConfig, model_module: CLIPModule, data_module: DataModule):\n",
    "        self.config = config\n",
    "        self.model_module = model_module\n",
    "        self.data_module = data_module\n",
    "        self.device = model_module.device\n",
    "\n",
    "        # Accuracy metrics\n",
    "        self.best_accuracy = 0.0\n",
    "\n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(patience=36, mode=\"min\", delta=0.01)\n",
    "\n",
    "    def train(self, train_dataset: Dataset, test_dataset: Dataset, train_indices: list[int], test_indices: list[int]):\n",
    "        \"\"\"\n",
    "        Complete training loop with validation.\n",
    "\n",
    "        Args:\n",
    "            train_dataset: Training dataset\n",
    "            test_dataset: Test/validation dataset\n",
    "            train_indices: Indices for training split\n",
    "            test_indices: Indices for test/validation split\n",
    "        \"\"\"\n",
    "        # Create dataloaders\n",
    "        train_dataloader, test_dataloader = self.data_module.create_dataloaders(train_dataset, test_dataset, train_indices, test_indices)\n",
    "\n",
    "        # Get training components\n",
    "        model = self.model_module.model\n",
    "        loss_img, loss_txt, optimizer = self.model_module.setup_training()\n",
    "\n",
    "        # Compute number of batches\n",
    "        num_batches_train = len(train_dataloader.dataset) / self.config.BATCH_SIZE\n",
    "        num_batches_test = len(test_dataloader.dataset) / self.config.BATCH_SIZE\n",
    "\n",
    "        # Accuracy metrics\n",
    "        num_classes = len(self.data_module.dataset.classes)\n",
    "        cumulative_correct_preds = torch.zeros(num_classes, dtype=torch.long)\n",
    "        cumulative_total_samples = torch.zeros(num_classes, dtype=torch.long)\n",
    "        cumulative_pred_labels = [] # Confusion matrix\n",
    "        cumulative_true_labels = [] # Confusion matrix\n",
    "\n",
    "        # self.early_stopping.reset()\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params({\n",
    "                \"batch_size\": self.config.BATCH_SIZE,\n",
    "                \"num_epochs\": self.config.NUM_EPOCHS,\n",
    "                \"learning_rate\": self.config.LEARNING_RATE,\n",
    "                \"weight_decay\": self.config.WEIGHT_DECAY\n",
    "            })\n",
    "\n",
    "            # Log random images as artifacts to review augmentation\n",
    "            [log_random_image(train_dataset, self.data_module.dataset.classes, counter=i) for i in range(1, 6)]\n",
    "\n",
    "            for epoch in range(self.config.NUM_EPOCHS):\n",
    "                print(f\"\\nEpoch {epoch+1}/{self.config.NUM_EPOCHS}\")\n",
    "\n",
    "                log_memory_usage(epoch+1, phase=\"Train - Start\") # DEV\n",
    "\n",
    "                # Training phase\n",
    "                epoch_train_loss = self._train_epoch(train_dataloader, model, optimizer, loss_img, loss_txt, num_batches_train)\n",
    "                print(f\"Epoch {epoch} train loss: {epoch_train_loss}\")\n",
    "                mlflow.log_metric(\"Loss/train\", epoch_train_loss, step=epoch)\n",
    "\n",
    "                # Save model checkpoint\n",
    "                if epoch % self.config.SAVE_INTERVAL == 0:\n",
    "                    checkpoint = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                        }\n",
    "                    checkpoint_path = Path(self.config.CHECKPOINT_SAVE_PATH) / f\"epoch_{epoch}.pt\"\n",
    "                    torch.save(checkpoint, checkpoint_path)\n",
    "                    print(f\"[Checkpoint] Saved under {checkpoint_path}\\n\")\n",
    "\n",
    "                log_memory_usage(epoch+1, phase=\"Train - End / Test - Start\") # DEV\n",
    "\n",
    "                # Testing phase\n",
    "                epoch_test_loss = self._test_epoch(\n",
    "                    test_dataloader, model, optimizer, loss_img, loss_txt, num_batches_test, cumulative_correct_preds, cumulative_total_samples, cumulative_pred_labels, cumulative_true_labels, epoch\n",
    "                    )\n",
    "                print(f\"Epoch {epoch} test loss: {epoch_test_loss}\\n\")\n",
    "                mlflow.log_metric(\"Loss/test\", epoch_test_loss, step=epoch)\n",
    "\n",
    "                log_memory_usage(epoch+1, phase=\"Test - End\") # DEV\n",
    "\n",
    "                # Early stopping\n",
    "                if self.early_stopping.early_stop:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                    break\n",
    "\n",
    "    def _train_epoch(self, train_dataloader: DataLoader, model: torch.nn.Module, optimizer: torch.optim.Optimizer, \n",
    "                     loss_img: torch.nn.Module, loss_txt: torch.nn.Module, num_batches_train: int) -> float:\n",
    "        \"\"\"\n",
    "        Training loop for a single epoch.\n",
    "\n",
    "        Args:\n",
    "            train_dataloader: Training dataloader\n",
    "            model: CLIP model\n",
    "            optimizer: Optimizer\n",
    "            loss_img: Image loss function\n",
    "            loss_txt: Text loss function\n",
    "            num_batches_train: Number of training batches\n",
    "        \n",
    "        Returns:\n",
    "            Average training loss for the epoch\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, total=num_batches_train):\n",
    "            optimizer.zero_grad() # Clear gradients from previous iteration\n",
    "\n",
    "            images, label_ids = batch\n",
    "\n",
    "            images = torch.stack([img for img in images], dim=0).to(self.device) # Stack images into a single tensor (adds an extra dim representing the batch)\n",
    "            # Generate text prompts: the number of text prompts will be equal to the number of images in the batch (label_ids)\n",
    "            # Scenario: matching each image with its corresponding text prompt, doesn't allow for comparison against other text prompts\n",
    "            # texts = [f\"A photo of a {train_dataset.dataset.classes[label_id]}\" for label_id in label_ids]\n",
    "            texts = [f\"A photo of a {self.data_module.dataset.classes[label_id]}\" for label_id in label_ids]\n",
    "            text = clip.tokenize(texts).to(self.device) # Tokenize text prompts \n",
    "            \n",
    "            logits_per_image, logits_per_text = model(images, text) # Forward pass\n",
    "\n",
    "            # Ground truth labels: For each batch, the i-th image corresponds to the i-th text\n",
    "            # Therefore, the i-th image should have the same label as the i-th text, i.e. [0, 1, 2, ..., BATCH_SIZE - 1]\n",
    "            # The same happens for text, so we use the same ground truth for both image and text\n",
    "            ground_truth = torch.arange(logits_per_image.shape[0], dtype=torch.long, device=self.device)\n",
    "\n",
    "            # Compute loss\n",
    "            total_train_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "            total_train_loss.backward() # Backward pass\n",
    "            epoch_train_loss += total_train_loss\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients to prevent exploding gradients\n",
    "\n",
    "            if self.device == \"cpu\":\n",
    "                optimizer.step() # Update weights\n",
    "            else:\n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step() # Update weights\n",
    "                clip.model.convert_weights(model)\n",
    "\n",
    "        # Average loss per epoch\n",
    "        return epoch_train_loss / num_batches_train\n",
    "    \n",
    "\n",
    "    def _test_epoch(self, test_dataloader: DataLoader, model: torch.nn.Module, optimizer: torch.optim.Optimizer, \n",
    "                    loss_img: torch.nn.Module, loss_txt: torch.nn.Module, num_batches_test: int, \n",
    "                    cumulative_correct_preds: torch.Tensor, cumulative_total_samples: torch.Tensor, \n",
    "                    cumulative_pred_labels: list[torch.Tensor], cumulative_true_labels: list[torch.Tensor], epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        Validate the model and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            test_dataloader: Test dataloader\n",
    "            model: CLIP model\n",
    "            optimizer: Optimizer\n",
    "            loss_img: Image loss function\n",
    "            loss_txt: Text loss function\n",
    "            num_batches_test: Number of test batches\n",
    "            cumulative_correct_preds: Cumulative number of correct predictions\n",
    "            cumulative_total_samples: Cumulative number of samples\n",
    "            cumulative_pred_labels: Cumulative list of predicted labels\n",
    "            cumulative_true_labels: Cumulative list of true labels\n",
    "            epoch: Current epoch number\n",
    "            \n",
    "        Returns:\n",
    "            Average test loss for the epoch\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        # Accurary metrics\n",
    "        acc_top3_list = []\n",
    "        acc_top1_list = []\n",
    "        all_pred_labels = []\n",
    "        all_true_labels = []\n",
    "\n",
    "        num_classes = len(self.data_module.dataset.classes)\n",
    "        classes = torch.arange(num_classes, device=self.device)\n",
    "\n",
    "        for i, batch in enumerate(tqdm(test_dataloader, total=num_batches_test)):\n",
    "            images, label_ids = batch\n",
    "            images = images.to(self.device)\n",
    "            label_ids = label_ids.to(self.device)\n",
    "\n",
    "            # Generate text prompts: the number of text prompts will be equal to the number of classes in the dataset (classes)\n",
    "            # Scenario: classifying each image against all possible classes, allows for comparison against all classes\n",
    "            texts = torch.cat([clip.tokenize(f\"A photo of a {c}\") for c in self.data_module.dataset.classes]).to(self.device) # Concatenate text prompts\n",
    "       \n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(texts)\n",
    "\n",
    "                logits_per_image, logits_per_text = model(images, texts) # Forward pass\n",
    "\n",
    "                # Ground truths\n",
    "                ground_truth_img = torch.arange(logits_per_image.shape[0], dtype=torch.long, device=self.device)\n",
    "                ground_truth_txt = -1 * torch.ones(len(classes), dtype=torch.long, device=self.device)\n",
    "                for idx, class_label in enumerate(classes):\n",
    "                    if class_label in label_ids:\n",
    "                        ground_truth_txt[idx] = (label_ids == class_label).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "                # Compute loss\n",
    "                img_loss = loss_img(logits_per_image, ground_truth_img)\n",
    "                txt_loss = loss_txt(logits_per_text, ground_truth_txt)\n",
    "                total_loss = (img_loss + txt_loss) / 2\n",
    "                epoch_test_loss += total_loss\n",
    "\n",
    "                # Normalize features\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                assert torch.equal(logits_per_image.T, logits_per_text), \"Logits are not equal\"\n",
    "\n",
    "                # Compute cosine similarity\n",
    "                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "                # [top acc] Compute top accuracy\n",
    "                acc_top1 = torchmetrics.functional.accuracy(similarity, label_ids, task=\"multiclass\", num_classes=num_classes)\n",
    "                acc_top3 = torchmetrics.functional.accuracy(similarity, label_ids, task=\"multiclass\", num_classes=num_classes, top_k=3)\n",
    "                acc_top1_list.append(acc_top1)\n",
    "                acc_top3_list.append(acc_top3)\n",
    "\n",
    "                # [per-class acc] Collect predictions and labels\n",
    "                predicted_labels = similarity.argmax(dim=-1)\n",
    "                all_pred_labels.append(predicted_labels)\n",
    "                all_true_labels.append(label_ids)\n",
    "\n",
    "                # [confusion matrix] Collect predictions and labels\n",
    "                cumulative_pred_labels.append(torch.cat(all_pred_labels))\n",
    "                cumulative_true_labels.append(torch.cat(all_true_labels))\n",
    "\n",
    "        # [per-class acc] Overall predictions, labels, and accuracy\n",
    "        all_pred_labels = torch.cat(all_pred_labels)\n",
    "        all_true_labels = torch.cat(all_true_labels)\n",
    "\n",
    "        # [confusion matrix]\n",
    "        accumulated_pred_labels = torch.cat(cumulative_pred_labels)\n",
    "        accumulated_true_labels = torch.cat(cumulative_true_labels)\n",
    "\n",
    "        # [confusion matrix] Log confusion matrix\n",
    "        log_confusion_matrix(accumulated_true_labels, accumulated_pred_labels, self.data_module.dataset.classes, epoch)\n",
    "\n",
    "        # [per-class acc] Update cumulative per-class counters\n",
    "        for class_idx in range(num_classes):\n",
    "            class_mask = (all_true_labels == class_idx)\n",
    "            correct_class_preds = (all_pred_labels[class_mask] == all_true_labels[class_mask]).sum().item()\n",
    "            total_class_samples = class_mask.sum().item()\n",
    "\n",
    "            cumulative_correct_preds[class_idx] += correct_class_preds\n",
    "            cumulative_total_samples[class_idx] += total_class_samples\n",
    "\n",
    "        # [per-class acc] Calculate and log cumulative per-class accuracy\n",
    "        class_accs = [] # Store class accuracies for averaging\n",
    "        for class_idx, class_name in enumerate(self.data_module.dataset.classes):\n",
    "            if cumulative_total_samples[class_idx] > 0:\n",
    "                accuracy = (cumulative_correct_preds[class_idx].item() / cumulative_total_samples[class_idx].item()) * 100\n",
    "                class_accs.append(accuracy)\n",
    "            else:\n",
    "                accuracy = float(\"nan\")\n",
    "            print(f\"Cumulative Accuracy for {class_name}: {accuracy:.2f}%\")\n",
    "            mlflow.log_metric(f\"Cumulative Accuracy/{class_name}\", accuracy, step=epoch)\n",
    "\n",
    "        # [top acc] Compute mean top3 and top1 accuracy\n",
    "        mean_top3_accuracy = torch.stack(acc_top3_list).mean().cpu().numpy()\n",
    "        print(f\"\\nMean Top 3 Accuracy: {mean_top3_accuracy*100:.2f}%\")\n",
    "        mlflow.log_metric(\"Test Accuracy/Top3\", mean_top3_accuracy, step=epoch)\n",
    "        mean_top1_accuracy = torch.stack(acc_top1_list).mean().cpu().numpy()\n",
    "        print(f\"Mean Top 1 Accuracy: {mean_top1_accuracy*100:.2f}%\")\n",
    "        mlflow.log_metric(\"Test Accuracy/Top1\", mean_top1_accuracy, step=epoch)\n",
    "\n",
    "        # [macro-avg acc] Compute macro avg accuracy\n",
    "        macro_accuracy = sum(class_accs) / len(class_accs) if class_accs else float(\"nan\")\n",
    "        print(f\"Macro-averaged Accuracy: {macro_accuracy:.2f}%\")\n",
    "        mlflow.log_metric(\"Test Accuracy/Macro-avg\", macro_accuracy, step=epoch)\n",
    "\n",
    "        # Save best model if macro average accuracy improves\n",
    "        self.early_stopping(macro_accuracy, model, optimizer, self.config.best_model_path, epoch)\n",
    "\n",
    "        # Average loss per epoch\n",
    "        return epoch_test_loss / num_batches_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b155d871-576f-454f-b279-dd7816458433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training Pipeline\n",
    "\n",
    "#### Main Entry Point\n",
    "\n",
    "Main script for initializing and executing the CLIP model training pipeline with dataset preparation and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9784b3-f323-40c1-8452-895968f89bff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Main\n",
    "# ----------\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def run_training():\n",
    "    \"\"\"\n",
    "    Runs complete CLIP training pipeline.\n",
    "\n",
    "    Handles:\n",
    "        - Configuration and module initialization\n",
    "        - Dataset loading and splitting\n",
    "        - Data augmentation and oversampling\n",
    "        - Training and validation\n",
    "    \"\"\"\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Init modules\n",
    "    config = TrainingConfig()\n",
    "    data_module = DataModule(config)\n",
    "    model_module = CLIPModule(config)\n",
    "\n",
    "    # Load model and dataset\n",
    "    model, preprocess = model_module.load_model()\n",
    "    dataset = data_module.load_dataset(preprocess)\n",
    "                                       \n",
    "    # Split dataset into train/test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_indices, test_indices = random_split(range(len(dataset)), [train_size, test_size])\n",
    "\n",
    "    # Prepare datasets with augmentation\n",
    "    train_dataset, test_dataset = data_module.prepare_data(dataset, train_indices, test_indices, target_samples_per_class=10)\n",
    "\n",
    "    # Display train/test dataset distribution\n",
    "    train_labels, test_labels = data_module.get_labels(train_indices, test_indices)\n",
    "    print(f\"All of the dataset's classes: {dataset.classes}\\n\")\n",
    "    class_distribution(labels=train_labels, class_names=dataset.classes, dataset_name=\"Train (before augmentation)\")\n",
    "    class_distribution(labels=test_labels, class_names=dataset.classes, dataset_name=\"Test (before augmentation)\")\n",
    "\n",
    "    # Oversample + Augmentation class distribution\n",
    "    # class_distribution(dataset=train_dataset, class_names=dataset.classes, dataset_name=\"Train (after augmentation)\")\n",
    "    # class_distribution(dataset=test_dataset, class_names=dataset.classes, dataset_name=\"Test (after augmentation)\")\n",
    "\n",
    "    # Regular training\n",
    "    trainer = CLIPTrainer(config, model_module, data_module)\n",
    "    trainer.train(train_dataset, test_dataset, train_indices, test_indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c98fa31-eda4-4a64-8c24-843b98eca6c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Before Augmentation ----\n",
      "Train dataset size: 74\n",
      "Test dataset size: 19\n",
      "\n",
      "Class chopping-board: 17 samples (No augmentation needed)\n",
      "Class glass-bowl-large: 1 samples -> Target: 10\n",
      "Class glass-bowl-medium: 1 samples -> Target: 10\n",
      "Class glass-bowl-small: 2 samples -> Target: 10\n",
      "Class group_step: 8 samples -> Target: 10\n",
      "Class oven-dish: 4 samples -> Target: 10\n",
      "Class oven-tray: 10 samples (No augmentation needed)\n",
      "Class pan: 20 samples (No augmentation needed)\n",
      "Class pot-one-handle: 4 samples -> Target: 10\n",
      "Class pot-two-handles-medium: 2 samples -> Target: 10\n",
      "Class pot-two-handles-small: 5 samples -> Target: 10\n",
      "---- After Augmentation ----\n",
      "Train dataset size: 127\n",
      "Test dataset size: 19\n",
      "\n",
      "All of the dataset's classes: ['chopping-board', 'glass-bowl-large', 'glass-bowl-medium', 'glass-bowl-small', 'group_step', 'oven-dish', 'oven-tray', 'pan', 'pot-one-handle', 'pot-two-handles-medium', 'pot-two-handles-small']\n",
      "\n",
      "Train (before augmentation) dataset -- Class distribution:\n",
      "pan -- 27.03%\n",
      "chopping-board -- 22.97%\n",
      "oven-tray -- 13.51%\n",
      "group_step -- 10.81%\n",
      "pot-two-handles-small -- 6.76%\n",
      "oven-dish -- 5.41%\n",
      "pot-one-handle -- 5.41%\n",
      "glass-bowl-small -- 2.70%\n",
      "pot-two-handles-medium -- 2.70%\n",
      "glass-bowl-large -- 1.35%\n",
      "glass-bowl-medium -- 1.35%\n",
      "\n",
      "\n",
      "Test (before augmentation) dataset -- Class distribution:\n",
      "pan -- 36.84%\n",
      "oven-tray -- 21.05%\n",
      "chopping-board -- 15.79%\n",
      "group_step -- 10.53%\n",
      "glass-bowl-small -- 5.26%\n",
      "oven-dish -- 5.26%\n",
      "pot-one-handle -- 5.26%\n",
      "\n",
      "\n",
      "Train (after augmentation) dataset -- Class distribution:\n",
      "pan -- 15.75%\n",
      "chopping-board -- 13.39%\n",
      "glass-bowl-large -- 7.87%\n",
      "glass-bowl-medium -- 7.87%\n",
      "glass-bowl-small -- 7.87%\n",
      "group_step -- 7.87%\n",
      "oven-dish -- 7.87%\n",
      "oven-tray -- 7.87%\n",
      "pot-one-handle -- 7.87%\n",
      "pot-two-handles-medium -- 7.87%\n",
      "pot-two-handles-small -- 7.87%\n",
      "\n",
      "\n",
      "Test (after augmentation) dataset -- Class distribution:\n",
      "pan -- 36.84%\n",
      "oven-tray -- 21.05%\n",
      "chopping-board -- 15.79%\n",
      "group_step -- 10.53%\n",
      "glass-bowl-small -- 5.26%\n",
      "oven-dish -- 5.26%\n",
      "pot-one-handle -- 5.26%\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/2\n",
      "[Train - Start Epoch 0] CPU Memory: RSS=2.50GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/7.9375 [00:00<?, ?it/s]\r 13%|        | 1/7.9375 [00:11<01:17, 11.18s/it]\r 25%|       | 2/7.9375 [00:20<01:00, 10.26s/it]\r 38%|      | 3/7.9375 [00:30<00:49, 10.02s/it]\r 50%|     | 4/7.9375 [00:40<00:38,  9.83s/it]\r 63%|   | 5/7.9375 [00:50<00:29, 10.06s/it]\r 76%|  | 6/7.9375 [01:00<00:19,  9.89s/it]\r 76%|  | 6/7.9375 [01:00<00:19, 10.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 1.2433167695999146\n",
      "[Checkpoint] Saved under <PATH>/recipe_classifier/checkpoints/2025_01_06/dataset_sample/epoch_0.pt\n",
      "\n",
      "[Train - End / Test - Start Epoch 0] CPU Memory: RSS=5.68GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1.1875 [00:00<?, ?it/s]\r 84%| | 1/1.1875 [00:06<00:01,  6.23s/it]\r2it [00:11,  5.84s/it]                            \r2it [00:11,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Accuracy for chopping-board: 100.00%\n",
      "Cumulative Accuracy for glass-bowl-large: nan%\n",
      "Cumulative Accuracy for glass-bowl-medium: nan%\n",
      "Cumulative Accuracy for glass-bowl-small: 100.00%\n",
      "Cumulative Accuracy for group_step: 0.00%\n",
      "Cumulative Accuracy for oven-dish: 0.00%\n",
      "Cumulative Accuracy for oven-tray: 100.00%\n",
      "Cumulative Accuracy for pan: 50.00%\n",
      "Cumulative Accuracy for pot-one-handle: 0.00%\n",
      "Cumulative Accuracy for pot-two-handles-medium: nan%\n",
      "Cumulative Accuracy for pot-two-handles-small: nan%\n",
      "\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 50.00%\n",
      "Macro-averaged Accuracy: 50.00%\n",
      "Epoch 0 test loss: 4.3275651931762695\n",
      "\n",
      "[Test - End Epoch 0] CPU Memory: RSS=5.69GB\n",
      "\n",
      "Epoch 2/2\n",
      "[Train - Start Epoch 1] CPU Memory: RSS=5.69GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/7.9375 [00:00<?, ?it/s]\r 13%|        | 1/7.9375 [00:09<01:04,  9.25s/it]\r 25%|       | 2/7.9375 [00:18<00:56,  9.43s/it]\r 38%|      | 3/7.9375 [00:28<00:46,  9.52s/it]\r 50%|     | 4/7.9375 [00:37<00:37,  9.54s/it]\r 63%|   | 5/7.9375 [00:47<00:27,  9.47s/it]\r 76%|  | 6/7.9375 [00:56<00:18,  9.48s/it]\r 76%|  | 6/7.9375 [00:56<00:18,  9.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 1.1142325401306152\n",
      "[Train - End / Test - Start Epoch 1] CPU Memory: RSS=6.55GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1.1875 [00:00<?, ?it/s]\r 84%| | 1/1.1875 [00:05<00:00,  5.00s/it]\r2it [00:10,  5.12s/it]                            \r2it [00:10,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Accuracy for chopping-board: 100.00%\n",
      "Cumulative Accuracy for glass-bowl-large: nan%\n",
      "Cumulative Accuracy for glass-bowl-medium: nan%\n",
      "Cumulative Accuracy for glass-bowl-small: 100.00%\n",
      "Cumulative Accuracy for group_step: 0.00%\n",
      "Cumulative Accuracy for oven-dish: 0.00%\n",
      "Cumulative Accuracy for oven-tray: 75.00%\n",
      "Cumulative Accuracy for pan: 50.00%\n",
      "Cumulative Accuracy for pot-one-handle: 0.00%\n",
      "Cumulative Accuracy for pot-two-handles-medium: nan%\n",
      "Cumulative Accuracy for pot-two-handles-small: nan%\n",
      "\n",
      "Mean Top 3 Accuracy: 85.71%\n",
      "Mean Top 1 Accuracy: 42.86%\n",
      "Macro-averaged Accuracy: 46.43%\n",
      "Epoch 1 test loss: 4.722964763641357\n",
      "\n",
      "[Test - End Epoch 1] CPU Memory: RSS=6.55GB\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9826195-a26f-43be-b831-02976e3755e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Saving Utils\n",
    "Utilities for saving complete CLIP models with architecture and creating lightweight checkpoints for deployment.\n",
    "\n",
    "##### Complete Model Saving\n",
    "\n",
    "Saves the full model architecture, weights, and preprocessing pipeline.\n",
    "\n",
    "- When loading the complete model:\n",
    "\n",
    "```python\n",
    "# Load everything at once\n",
    "checkpoint = torch.load('complete_model.pth')\n",
    "model = checkpoint['model']\n",
    "preprocess = checkpoint['preprocess']\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "##### Lightweight Model Saving\n",
    "\n",
    "Saves only essential model weights (model state dictionary) for lightweight deployments. Excludes the optimizer state dictionary (only useful for training) to minize file size.\n",
    "\n",
    "- When loading the model state dictionary:\n",
    "\n",
    "```python\n",
    "# Initialize model architecture first\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "else:\n",
    "    clip.model.convert_weights(model)\n",
    "\n",
    "# Load state dict\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1af66c1e-ab4b-46fa-bd53-82124d8e49e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Complete Model Saving\n",
    "\n",
    "# Save complete model with architecture\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "else:\n",
    "    clip.model.convert_weights(model)\n",
    "\n",
    "# Load the fine-tuned weights\n",
    "root_dir: str = \"<PATH>\"\n",
    "CHECKPOINT_SAVE_PATH = f\"{root_dir}recipe_classifier/checkpoints/2025_01_09/dataset_20dec_small/grid_search/lr_1e-05_wd_0/\"\n",
    "checkpoint_path = f\"{CHECKPOINT_SAVE_PATH}best_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Save complete model\n",
    "torch.save({\n",
    "    \"model\": model,\n",
    "    \"preprocess\": preprocess\n",
    "}, f\"{CHECKPOINT_SAVE_PATH}complete_best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb937f36-e743-4976-a328-864944c0f3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lightweight Model Saving\n",
    "\n",
    "# Convert complete model (including model_state_dict and optimizer_state_dict) to only include model_state_dict\n",
    "\n",
    "root_dir: str = \"<PATH>\"\n",
    "CHECKPOINT_SAVE_PATH = f\"{root_dir}recipe_classifier/checkpoints/2025_01_18/dataset_20dec/b32/\"\n",
    "checkpoint_path = f\"{CHECKPOINT_SAVE_PATH}best_model.pth\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(f\"Checkpoint keys:\\n\", checkpoint.keys())\n",
    "\n",
    "lightweight_checkpoint = {\n",
    "    \"epoch\": checkpoint[\"epoch\"],\n",
    "    \"model_state_dict\": checkpoint[\"model_state_dict\"],\n",
    "    \"metric\": checkpoint[\"metric\"],\n",
    "}\n",
    "\n",
    "torch.save(lightweight_checkpoint, f\"{CHECKPOINT_SAVE_PATH}lightweight_best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6798c9-085f-4226-b4c1-d3919ff618b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "Run inference with a fine-tuned CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974ef03e-501a-4c3c-914e-444a70c0b635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# Inference\n",
    "# ----------\n",
    "\n",
    "# 1. Model Setup\n",
    "#   - Set available device\n",
    "#   - Load base CLIP architecture\n",
    "#   - Load fine-tuned weights from checkpoint\n",
    "#   - Set model to evaluation mode\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Load the fine-tuned weights\n",
    "CHECKPOINT_SAVE_PATH = f\"{root_dir}recipe_classifier/checkpoints/2024_12_02/dataset_sample/\"\n",
    "checkpoint_path = f\"{CHECKPOINT_SAVE_PATH}best_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# 2. Image Processing\n",
    "#   - Load target image\n",
    "#   - Apply CLIP preprocessing transform\n",
    "#   - Add batch dimension and move to device\n",
    "\n",
    "# Load image\n",
    "image_path = f\"{root_dir}recipe_classifier/dataset/tests/test_group.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess image\n",
    "image = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# 3. Text Prompt Generation\n",
    "#   - Define a list pof possible class keywords\n",
    "#   - Convert keywords to CLIP-style prompts (\"A photo of a {keyword}\")\n",
    "#   - Tokenize prompts for model input and move to device\n",
    "\n",
    "# Generate text prompt\n",
    "keywords = [\"pan\",\n",
    "    \"oven-dish\",\n",
    "    \"grill-plate\",\n",
    "    \"oven-tray\",\n",
    "    \"chopping-board\",\n",
    "    \"medium\", # chopping-board + medium\n",
    "    \"CP\", # chopping-board + CP\n",
    "    \"grill-tray\",\n",
    "    \"pot-two-handles-medium\",\n",
    "    \"pot-two-handles-small\",\n",
    "    \"pot-two-handles-shallow\",\n",
    "    \"pot-one-handle\",\n",
    "    #\"sauce-pan\",\n",
    "    \"saucepan\",\n",
    "    \"glass-bowl-large\",\n",
    "    \"glass-bowl-medium\",\n",
    "    \"glass-bowl-small\",\n",
    "    \"finalstep\",\n",
    "    \"group_step\",\n",
    "]\n",
    "text_prompts = [f\"A photo of a {keyword}\" for keyword in keywords]\n",
    "tokenized_text = clip.tokenize(text_prompts).to(device)\n",
    "\n",
    "# 4. Feature Extraction\n",
    "#   - Extract image features from preprocessed image\n",
    "#   - Extract text features from tokenized prompts\n",
    "#   - Normalizes both feature sets for comparison\n",
    "\n",
    "# Generate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True) # Normalize features\n",
    "    \n",
    "    text_features = model.encode_text(tokenized_text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # Normalize features\n",
    "\n",
    "# 5. Prediction\n",
    "#   - Compute cosine similarity between image and text features\n",
    "#   - Apply softmax to get probability distribution\n",
    "#   - Get top prediction and probability\n",
    "#   - Map prediction index back to keyword\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Get predicted keyword\n",
    "predicted_prob, predicted_keyword_idx = similarity.topk(1, dim=-1)\n",
    "\n",
    "# Print prediction\n",
    "predicted_keyword = keywords[predicted_keyword_idx.item()]\n",
    "print(f\"Predicted keyword: {predicted_keyword} with probability {predicted_prob.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b72e4ec-7a4b-421d-93af-68354072dd25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLflow Utils\n",
    "\n",
    "Utilities to perform operations in MLflow experiments and runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bbc15f-ac4a-41e8-bbed-4eb75e8e3bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active runs found.\n"
     ]
    }
   ],
   "source": [
    "def print_active_runs():\n",
    "    \"\"\"Lists all currently active MLflow runs across experiments.\"\"\"\n",
    "    try:\n",
    "        active_runs = mlflow.search_runs(filter_string=\"attributes.status = 'RUNNING'\")\n",
    "        \n",
    "        if len(active_runs) == 0:\n",
    "            print(\"No active runs found.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nFound {len(active_runs)} active run(s):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, run in active_runs.iterrows():\n",
    "            experiment = mlflow.get_experiment(run.experiment_id)\n",
    "            experiment_name = experiment.name if experiment else \"Unknown\"\n",
    "            print(f\"Run {idx + 1}:\")\n",
    "            print(f\"Experiment Name: {experiment_name}\")\n",
    "            print(f\"Run ID: {run.run_id}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching active runs: {e}\")\n",
    "\n",
    "print_active_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de00215e-3e12-4772-8ed5-3a13d00b28e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active runs to end.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def end_active_runs():\n",
    "    \"\"\"Finds and terminates all active MLflow runs.\"\"\"\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "\n",
    "        # Search for active runs\n",
    "        active_runs = mlflow.search_runs(filter_string=\"attributes.status = 'RUNNING'\")\n",
    "        \n",
    "        if len(active_runs) == 0:\n",
    "            print(\"No active runs to end.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nFound {len(active_runs)} active run(s):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Iterate through each active run and end it\n",
    "        for idx, run in active_runs.iterrows():\n",
    "            experiment = mlflow.get_experiment(run.experiment_id)\n",
    "            experiment_name = experiment.name if experiment else \"Unknown\"\n",
    "            print(f\"Ending Run {idx + 1}:\")\n",
    "            print(f\"Experiment Name: {experiment_name}\")\n",
    "            print(f\"Run ID: {run.run_id}\")\n",
    "            \n",
    "            # End the run using the client\n",
    "            client.set_terminated(run_id=run.run_id, status=\"FINISHED\")\n",
    "            print(\"Run ended successfully.\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error ending active runs: {e}\")\n",
    "\n",
    "end_active_runs()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RecipeClassifier Zero-Shot PoC v6 (Modules)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
